{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T03:54:27.654920Z",
     "start_time": "2022-02-04T03:54:27.649922Z"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import progressbar\n",
    "from dateutil.parser import parse\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T02:31:52.055806Z",
     "start_time": "2022-02-04T02:31:00.546994Z"
    }
   },
   "outputs": [],
   "source": [
    "import twint\n",
    "import nest_asyncio\n",
    "\n",
    "d_begin = \"2022-03-10\"\n",
    "d_end = \"2022-04-02\"\n",
    "daily_datarange = pd.date_range(start=d_begin,\n",
    "                                end=d_end ,\n",
    "                                freq='b')\n",
    "\n",
    "nest_asyncio.apply()\n",
    "tweet_ids = []\n",
    "tweet_urls  = []\n",
    "\n",
    "for start_i in range(0, len(daily_datarange), 1):\n",
    "    c = twint.Config()\n",
    "    c.Username = \"cnn\"      \n",
    "    c.Pandas = True\n",
    "    c.Since = str(daily_datarange[start_i].strftime(\"%Y-%m-%d\"))\n",
    "    c.Until = str((daily_datarange[start_i] + datetime.timedelta(days=2)).strftime(\"%Y-%m-%d\"))\n",
    "    c.Store_csv = True\n",
    "    c.Custom_csv = [\"id\", \"user_id\", \"username\", \"tweet\"]\n",
    "    c.Output = (\"C:/Users/13a71/Dropbox/github/crawler/tweeter crawler/twint/cnn/{}.csv\".format(c.Since))\n",
    "    c.Filter_retweets = True\n",
    "    # Run\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    \n",
    "    tweet_urls.extend([str(tweet.urls) for tweet in tweet_ids])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T02:43:34.977673Z",
     "start_time": "2022-02-04T02:43:34.102686Z"
    }
   },
   "outputs": [],
   "source": [
    "searchlist = pd.read_excel('~/documents/cnn_raw_tweet0402.xlsx').urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T03:54:36.644351Z",
     "start_time": "2022-02-04T03:54:36.639353Z"
    }
   },
   "outputs": [],
   "source": [
    "#Open/reset progressbar and dataframes\n",
    "# pbar = progressbar()\n",
    "\n",
    "#Create dataframe for storing article contents\n",
    "content_df = pd.DataFrame(columns=['date', 'title', 'category', 'author', 'content', 'url'])\n",
    "\n",
    "#Create dataframe for errorlist\n",
    "error_df = pd.DataFrame(columns=['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T03:54:37.546780Z",
     "start_time": "2022-02-04T03:54:37.537787Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from dateutil.parser import parse\n",
    "from bs4 import NavigableString\n",
    "\n",
    "def get_soup(url, headers=None):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    url : str\n",
    "        Web page url\n",
    "    headers : dict\n",
    "        Headers for requests. If None, use Mozilla/5.0 as default user-agent\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    soup : bs4.BeautifulSoup\n",
    "        Soup format web page\n",
    "    \"\"\"\n",
    "\n",
    "    if headers is None:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'}\n",
    "    r = requests.get(url, headers=headers)\n",
    "    html = r.text\n",
    "    page = BeautifulSoup(html, 'lxml')\n",
    "    return page\n",
    "\n",
    "doublespace_pattern = re.compile('\\s+')\n",
    "lineseparator_pattern = re.compile('\\n+')\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.replace('\\t', ' ')\n",
    "    text = text.replace('\\r', ' ')\n",
    "    text = lineseparator_pattern.sub('\\n', text)\n",
    "    text = doublespace_pattern.sub(' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def to_string(instance):\n",
    "    final_string = ''\n",
    "    if isinstance(instance, NavigableString):\n",
    "        return instance\n",
    "    for contents in instance.contents:\n",
    "        final_string += to_string(contents)\n",
    "    return final_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T03:54:38.495762Z",
     "start_time": "2022-02-04T03:54:38.487760Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## CNN SCRAPER\n",
    "def parse_article(url):\n",
    "    try:\n",
    "        soup = get_soup(url)\n",
    "    except:\n",
    "        print ('trying again in 10 seconds')\n",
    "        time.sleep(0.1)\n",
    "        soup = get_soup(url)\n",
    "    #Get Kicker (catergory)\n",
    "    try:\n",
    "        links_all= soup.find_all('link')\n",
    "        links = [url['href'] for url in links_all if \"https://www.cnn.com/\" in url['href']]\n",
    "        main_url = links[0]\n",
    "        kicker = main_url.split(\"/\")[6]\n",
    "    except:\n",
    "        kicker = \"\"\n",
    "\n",
    "    #Get the title\n",
    "    try:\n",
    "        title = soup.find('h1', class_='pg-headline').text\n",
    "    except:\n",
    "        print('title error')\n",
    "        \n",
    "    #Get date and strip\n",
    "    try:\n",
    "        date = '-'.join([str(elem) for elem in (main_url.split(\"/\")[3:6])]) \n",
    "    except:\n",
    "        date = ''\n",
    "\n",
    "    #Get author\n",
    "    try:\n",
    "        author = soup.find('span', class_ = 'metadata__byline__author').text\n",
    "    except:\n",
    "        author = ''\n",
    "\n",
    "    #Get body text and strip advertisement\n",
    "    try:\n",
    "        content = soup.find('div', class_ = 'pg-rail-tall__body').text.replace(\",\", \"\")\n",
    "    except:\n",
    "        content = \"\"\n",
    "    #print (body)\n",
    "    \n",
    "    json_object = {\n",
    "          'date': date,\n",
    "          'title': title,\n",
    "          'subtitle': '',\n",
    "          'content': content,\n",
    "        'author' :author,\n",
    "          'category': kicker,\n",
    "          'source': 'CNN',\n",
    "          'url': url\n",
    "      }\n",
    "    return json_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T04:28:14.798053Z",
     "start_time": "2022-02-04T03:54:42.938872Z"
    }
   },
   "outputs": [],
   "source": [
    "def save(json_obj, directory):\n",
    "    date = json_obj.get('date', '')\n",
    "    title = json_obj.get('title', '')\n",
    "    filepath = '{}/{}_{}.json'.format(directory, date, re.sub('[^a-zA-Z ]+',\"\", title[:50]))\n",
    "    with open(filepath, 'w', encoding='utf-8') as fp:\n",
    "        json.dump(json_obj, fp, indent=2, ensure_ascii=False)\n",
    "        print('scraped {}'.format(json_obj['title'][:10]))\n",
    "        \n",
    "## CNN SCRAPER\n",
    "directory = 'C:/Users/13a71/Documents/crawling output/cnn'\n",
    "    \n",
    "import json\n",
    "for url in searchlist:\n",
    "    url = url[2:-2]\n",
    "    try:\n",
    "        article = parse_article(url)\n",
    "        save(article, directory)\n",
    "    except:\n",
    "        print(\"this url error\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T02:26:12.699380Z",
     "start_time": "2021-11-12T02:26:12.673381Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T23:05:53.135642Z",
     "start_time": "2021-12-09T23:05:53.106648Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "content_df.dropna(subset = [\"date\"], inplace=True)\n",
    "content_df = content_df.drop_duplicates(subset='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df.to_excel('cnn_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T05:53:30.104158Z",
     "start_time": "2021-09-20T05:53:30.091157Z"
    }
   },
   "outputs": [],
   "source": [
    "all_df = pd.concat([all_df, content_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-09T23:05:57.254Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "content_df.to_excel('cnn.xlsx')\n",
    "\n",
    "error_df.to_csv('wp_peace2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dnjtldxjs vhtmxm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for url in pbar(urls):\n",
    "    url = url.split('?')[0] + '?outputType=amp'\n",
    "    try:\n",
    "        #Check if the url is a WP page and make some unscrapable exceptions\n",
    "        if \"washingtonpost\" not in url:\n",
    "            error_df.loc[len(content_df)] = [url]\n",
    "            continue\n",
    "        if \"/graphics/\" in url:\n",
    "            error_df.loc[len(content_df)] = [url]\n",
    "            continue          \n",
    "  \n",
    "            \n",
    "        try:\n",
    "            r = requests.get(url, headers=headers)\n",
    "        except:\n",
    "            try:\n",
    "                print ('trying again in 10 seconds')\n",
    "                time.sleep(10)\n",
    "                r = requests.get(url, headers=headers)\n",
    "            except:\n",
    "                print (str(url) + ' TIME OUT could not be scraped')\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        #Get the soup\n",
    "        soup = BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "        #Get Kicker (catergory)\n",
    "        try:\n",
    "            kicker = soup.find('div', 'kicker').text\n",
    "        except:\n",
    "            try:\n",
    "                kicker = soup.find('div', 'headline-kicker').text\n",
    "            except:\n",
    "                rint('No Kicker' + url)\n",
    "                kicker = \"\"\n",
    "\n",
    "        #Get the title\n",
    "        title = soup.find('h1').text\n",
    "        #print(title)\n",
    "\n",
    "        #Try to get the subtitle\n",
    "        try:\n",
    "            subtitle = soup.find('h2').text\n",
    "            if \"\\n\" in subtitle:\n",
    "                subtitle = \"\"\n",
    "        except:\n",
    "            #print('No subtitle'  + url)\n",
    "            subtitle = \"\"\n",
    "\n",
    "        #Get date and strip\n",
    "        try:\n",
    "            raw_date = soup.find('div', 'display-date').text\n",
    "            date = datetime.strptime(raw_date, \"%B %d, %Y at %I:%M %p EDT\")\n",
    "        except:\n",
    "            try:\n",
    "                raw_date = soup.find('span', 'author-timestamp')['content'][:10]\n",
    "                date = datetime.strptime(raw_date, \"%Y-%M-%d\")\n",
    "            except:\n",
    "                try:\n",
    "                    raw_date = soup.find('div', 'date')['content'][:16]\n",
    "                    date = datetime.strptime(raw_date, \"%Y-%M-%d %H:%M\")\n",
    "                except:\n",
    "                    try:\n",
    "                        raw_date = soup.find('div', 'display-date').text\n",
    "                        date = datetime.strptime(raw_date, \"%B %d, %Y at %I:%M %p EST\")\n",
    "                    except:\n",
    "                        print('No date'  + url)\n",
    "                        date = \"\"\n",
    "\n",
    "        #Get author\n",
    "        try:\n",
    "            author = soup.find('span', 'author-name').text.replace(\",\", \"\")\n",
    "        except:\n",
    "            author = soup.find('a', 'author-name').text.replace(\",\", \"\")\n",
    "\n",
    "        #Get body text and strip advertisement\n",
    "        try:\n",
    "            full_body = soup.find('div', 'article-body')\n",
    "            filtered_body = \"\"\n",
    "            for i in full_body.find_all('p', class_='font-copy'): \n",
    "                #if (not 'hide-for-print' in str(i) and not 'Read more:' in str(i)):\n",
    "                if not 'hide-for-print' in str(i):\n",
    "                    filtered_body += \" \" + i.text\n",
    "            body = filtered_body.split('Read more')[0]\n",
    "        except:\n",
    "            full_body = soup.find('article', 'pg-article')\n",
    "            body = \"\"\n",
    "            for i in full_body.find_all('p', class_='pg-body-copy'): \n",
    "                body += \" \" + i.text\n",
    "        #print (body)\n",
    "        \n",
    "        content_df.loc[len(content_df)] = [date, title, subtitle, kicker, author, body.replace(\",\", \"\"), url]\n",
    "    \n",
    "    except:\n",
    "        error_df.loc[len(content_df)] = [url]\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T07:38:27.516438Z",
     "start_time": "2021-08-07T07:38:27.487253Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T15:44:13.490563Z",
     "start_time": "2021-08-07T15:44:13.379563Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "content_df = content_df.loc[content_df[\"date\"] >= \"2021-07-24\"].reset_index(drop=True)\n",
    "content_df = content_df.loc[content_df[\"date\"] <= \"2021-08-04\"].reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "468px",
    "left": "1495px",
    "right": "20px",
    "top": "629px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
