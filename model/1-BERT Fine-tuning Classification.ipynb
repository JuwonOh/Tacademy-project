{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T08:20:57.657771Z",
     "start_time": "2022-02-04T08:20:57.428771Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import os\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:57:24.114420Z",
     "start_time": "2022-02-02T01:57:24.052729Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VERSION CHECK\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration & Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. bert-base-uncased\n",
    "- 12-layer, 768-hidden, 12-heads, 110M parameters.\n",
    "- Trained on lower-cased English text.\n",
    "\n",
    "2. bert-large-uncased\n",
    "- 24-layer, 1024-hidden, 16-heads, 340M parameters.\n",
    "- Trained on lower-cased English text.\n",
    "\n",
    "3. bert-base-cased\n",
    "- 12-layer, 768-hidden, 12-heads, 110M parameters.\n",
    "- Trained on cased English text.\n",
    "\n",
    "4. bert-large-cased\n",
    "- 24-layer, 1024-hidden, 16-heads, 340M parameters.\n",
    "- Trained on cased English text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### major error issue\n",
    " \n",
    "- error issue: dropout(): argument 'input' (position 1) must be Tensor, not str when using Bert with Huggingface\n",
    "- transformers version issue: transformers 3.0에서 transformers 4.x버전으로 옮기면서 나타나는 문제.\n",
    "    \n",
    "- 해법: 모델 함수에 return_dict=False\n",
    "    \n",
    "reference: https://huggingface.co/docs/transformers/migration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:57:28.445896Z",
     "start_time": "2022-02-02T01:57:24.155472Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 14.6kB/s]\n",
      "Downloading: 100%|██████████| 208k/208k [00:00<00:00, 511kB/s] \n",
      "Downloading: 100%|██████████| 426k/426k [00:00<00:00, 559kB/s]  \n"
     ]
    }
   ],
   "source": [
    "# 다른 pretrained model을 사용하고 싶다면 위에서 원하는 이름을 선택해서 아래 변수명을 변경한다.\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=False) \n",
    "BATCH_SIZE = 8\n",
    "tag = 'sentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:57:28.476935Z",
     "start_time": "2022-02-02T01:57:28.461916Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(tag):\n",
    "    \"\"\"\n",
    "    Description: 본 데이터 셋에서는 응답자가 의미없다는 답변을 할 수 있다. \n",
    "    의미 없다는 답변을 받은 데이터는 'irrelevant', 의미있는 답변이 담긴 데이터 프레임을 받은 데이터 프레임은 'sentiment'로 선택한다.\n",
    "    ---------\n",
    "    Arguments\n",
    "    ---------\n",
    "    tag : str\n",
    "        'irrelevant', 'sentiment'로 원하는 데이터를 뽑아낸다.\n",
    "    ---------    \n",
    "    Return: pandas.Dataframe\n",
    "            \n",
    "    ---------\n",
    "    \n",
    "    \"\"\"\n",
    "    if tag == 'irrelevant':\n",
    "        train_df = pd.read_csv('data/irrelevant_train.tsv', sep='\\t')\n",
    "        valid_df = pd.read_csv('data/irrelevant_test.tsv', sep='\\t')\n",
    "    elif tag == 'sentiment':\n",
    "        train_df = pd.read_csv('data/sentiment_train.tsv', sep='\\t')\n",
    "        valid_df = pd.read_csv('data/sentiment_test.tsv', sep='\\t')\n",
    "    else:\n",
    "        train_df = None\n",
    "        valid_df = None\n",
    "    \n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:57:28.508386Z",
     "start_time": "2022-02-02T01:57:28.493953Z"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Description: 불러온 데이터프레임에서 필요한 정보를 뽑아내고, 임베딩한다.\n",
    "    ---------\n",
    "    Arguments\n",
    "    ---------\n",
    "    sentences: str\n",
    "        문장에 대한 정보를 저장\n",
    "    labels : int\n",
    "        들어온 문장의 id를 가지고 있는다.\n",
    "    tokenizer: str\n",
    "        어떤 tokenizer를 사용할지 지정한다.\n",
    "    max_len: int\n",
    "        문장의 최대 길이를 지정하여 지나치게 긴 문장을 잘라낸다. max_len이 길어지면, padding을 해야하기에 연산이 느려진다.\n",
    "    ---------    \n",
    "    Return: dict\n",
    "        dict안의 value와 item은 다음과 같다.\n",
    "    ---------\n",
    "    sentence: str\n",
    "        input에 들어갈 문장정보\n",
    "    input_ids: tensor\n",
    "        tokenizer가 임베딩한 문장의 정보\n",
    "    attention_mask: tensor\n",
    "        최대 길이로 지정한 512에서 단어가 차지하는 길이에 대한 정보        \n",
    "    labels: int\n",
    "        분류 모델이 분류할 타겟 변수\n",
    "    \"\"\"\n",
    "    def __init__(self, sentences, labels, tokenizer, max_len):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        sentence = str(self.sentences[item])\n",
    "        label = self.labels[item]\n",
    "        \n",
    "        ## torch가 인식 할 수 있도록 문장을 embedding한다.\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'sentence': sentence,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:57:28.761764Z",
     "start_time": "2022-02-02T01:57:28.524756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32887, 4) (8222, 4)\n"
     ]
    }
   ],
   "source": [
    "train_df, valid_df = load_dataset(tag)\n",
    "df_train, df_test = train_test_split(train_df, test_size=0.2, random_state=1234)\n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:57:28.809774Z",
     "start_time": "2022-02-02T01:57:28.794763Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = SentimentDataset(df_train.sentence.values, df_train.sentiment.values, tokenizer, max_len=512)\n",
    "## gpu가 감당 할 수 있게, small batch를 사용한다.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=0)\n",
    "\n",
    "test_dataset = SentimentDataset(df_test.sentence.values, df_test.sentiment.values, tokenizer, max_len=512)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-02T01:57:26.730Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "## data loader를 사용한다.\n",
    "data = next(iter(train_dataloader))\n",
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['labels'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-02T01:57:28.073Z"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Description: torch의 nn.Module을 사용해서 분류기 클래스를 만든다.\n",
    "    ---------\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-02T01:57:28.938Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "## bert모델 를 정의한다. class는 2개.\n",
    "model = SentimentClassifier(2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-02T01:57:29.354Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50 ## 바꿀 수 있는 parameter\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "## lr를 줄이기 위해서 scheduler를 사용한다.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "## loss function을 정의한다.\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.optim.lr_scheduler.LambdaLR at 0x24c455be748>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-02T01:57:29.921Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    \"\"\"\n",
    "    Description: train을 해주는 모듈\n",
    "    ---------\n",
    "    Arguments\n",
    "    ---------\n",
    "    model: nn.module\n",
    "        정의한 모델\n",
    "    loss_fn : CrossEntropyLoss()\n",
    "        손실함수\n",
    "    tokenizer: PreTrainedTokenizer\n",
    "        앞에서 지정한 tokenizer\n",
    "    max_len: int\n",
    "        지정한 문장의 최대길이\n",
    "    optimizer: AdamW \n",
    "        사용하고자 하는 optimizer\n",
    "    device: device(type='cuda')\n",
    "        gpu사용 혹은 cpu사용\n",
    "    scheduler: scheduler\n",
    "        사용할 scheduler\n",
    "    n_examples : int\n",
    "        전체 분류기에 사용할 자료의 수\n",
    "    ---------    \n",
    "    Return: train_accuracy, train_loss\n",
    "    ---------\n",
    "    \"\"\"\n",
    "    ## model을 train mode로 변환.\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"labels\"].to(device)\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        ## gradient normalization\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-02T01:57:30.657Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    \"\"\"\n",
    "    Description: train된 모델을 evaluation을 해주는 모듈\n",
    "    ---------\n",
    "    Arguments\n",
    "    ---------\n",
    "    model: nn.module\n",
    "        정의한 모델\n",
    "    loss_fn : CrossEntropyLoss()\n",
    "        손실함수\n",
    "    device: device(type='cuda')\n",
    "        gpu사용 혹은 cpu사용\n",
    "    n_examples : int\n",
    "        전체 분류기에 사용할 자료의 수\n",
    "    ---------    \n",
    "    Return: eval_accuracy, eval_loss\n",
    "    ---------\n",
    "    \"\"\"\n",
    "    ## model을 eval mode로 변환.\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"labels\"].to(device)\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-02T01:57:31.546Z"
    }
   },
   "outputs": [],
   "source": [
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(df_train)\n",
    "    )\n",
    "    \n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        test_dataloader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(df_test)\n",
    "    )\n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}/{EPOCHS}] Train loss: {train_loss} acc: {train_acc} | Val loss: {val_loss} acc: {val_acc}')\n",
    "\n",
    "    print()\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    if val_acc > best_accuracy:\n",
    "        model_dir = 'model/'+tag+'_BERT_model.bin'\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "        torch.jit.save(torch.jit.script(model), model_dir)\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = SentimentDataset(valid_df.sentence.values, valid_df.sentiment.values, tokenizer, max_len=512)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = eval_model(model, valid_dataloader, loss_fn, device, len(valid_dataset))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and Get probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentClassifier(2)\n",
    "model.load_state_dict(torch.load('model/mobilebert_model_67.pt', map_location='cpu'))\n",
    "## 안되면 model.load_state_dict(torch.load('model/mobilebert_model_67.pt', map_location='cpu'), strict = False)\n",
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input_text, model):\n",
    "    \"\"\"\n",
    "    Description: 특정 문장이 들어오면 input text를 embedding하고, inference 해주는 모듈\n",
    "        api serving에서 사용.\n",
    "    ---------\n",
    "    Arguments\n",
    "    ---------\n",
    "    input_text: str\n",
    "        사용자가 넣을 문장 정보.\n",
    "    model: model\n",
    "        사용자가 지정한 모델 정보.\n",
    "    ---------    \n",
    "    Return: 0과 1 사이의 결과 값\n",
    "    ---------\n",
    "    \"\"\"\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "        review_text,\n",
    "        max_length=512,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "    \n",
    "    logits = model(input_ids, attention_mask)\n",
    "    softmax_prob = torch.nn.functional.softmax(logits, dim=1)\n",
    "    _, prediction = torch.max(softmax_prob, dim=1)\n",
    "    \n",
    "    return softmax_prob, prediction\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code\n",
    "review_text = \"any line of news to test code\"\n",
    "class_prob, pred = inference(review_text, model)\n",
    "print(class_prob.detach().cpu().numpy()[0])\n",
    "print(pred.detach().cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def save_results(df, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for idx, row in tqdm(df.iterrows()):\n",
    "            news_id = row['id']\n",
    "            text = row['sentence'].replace('\\t',' ')\n",
    "            sentiment = row['sentiment']\n",
    "            class_prob, pred = inference(text, model)\n",
    "\n",
    "            class_prob = [str(x) for x in class_prob.detach().cpu().numpy()[0]]\n",
    "            pred = pred.detach().cpu().numpy()[0]\n",
    "\n",
    "            result = str(news_id).replace('\\t','')+'\\t'+text+'\\t'+'\\t'.join(class_prob)+'\\t'+str(pred)+'\\t'+str(int(sentiment)).replace('\\t','')\n",
    "            \n",
    "            f.write(result+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(train_df, 'data/'+tag+'_bert_prediction_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(valid_df, 'data/'+tag+'_bert_prediction_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 보류: APEX\n",
    "\n",
    "### amp Mixed Precision\n",
    "\n",
    "float16으로 Type Casting 되는 것이 빠른 연산(Linear Layer, Conv Layer etc.)은 float16으로 변환해서 연산을 수행하는 것이 가능. \n",
    "float16으로 변환시키는 경우 autocast와 GradScaler를 사용한다. \n",
    "- autocast: autocast는 with 문과 함께 선언해서 사용하면, with내부의 토치 연산들은 mixed precision로 실행된다. model의 forward 연산과, loss 계산 연산을 with문 아래에 위치 시켜야 한다.\n",
    "- GradScaler: float16으로 연산하면, foward-pass 이후의 backward-pass에서 그라디언트가 너무 작아져서 underflow가 일어나기도 한다. GradScaler는 이를 방지하기 위해 backward시 float32로 실행한다.\n",
    "\n",
    "- amp은 float16연산 이외에도 DataParallel 기능을 지원한다. 특히 여러 gpu를 동시에 사용할때 DistributedDataParallel을 통해 하나의 작업에 여러 gpu를 사용할 수 있다. \n",
    "\n",
    "#### 결과 기록\n",
    "- APEX를 사용했을때, 제대로 학습이 되지 않고 전부 1로 LABALING한다.\n",
    "- 이 원인에 대해서 찾아볼 것.\n",
    "\n",
    "#### reference\n",
    "- amp module에 대한 torch 공식 튜토리얼: https://pytorch.org/docs/stable/amp.html\n",
    "- amp module의 각 method에 대한 설명: https://pytorch.org/docs/stable/notes/amp_examples.html\n",
    "- amp에 대한 한글 번역본: https://runebook.dev/ko/docs/pytorch/amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50 ## 바꿔야할 파라미터\n",
    "\n",
    "use_amp = True\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "## GradScaler를 넣는 부분.\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=3,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    for d in data_loader:\n",
    "        ## autocast를 넣어주는 부분.\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"labels\"].to(device)\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "        scaler.scale(loss).backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  DYNAMIC QUANTIZATION\n",
    "\n",
    "- 참고자료  \n",
    "    - BERT 모델 동적 양자화하기: https://tutorials.pytorch.kr/intermediate/dynamic_quantization_bert_tutorial.html\n",
    "\n",
    "- 의문사항\n",
    "    - AMP와 qunatization을 동시에 사용하면 안되나? \n",
    "    - amp에 대한 공식 설명: https://pytorch.org/docs/stable/amp.html\n",
    "    - amp의 기능에 대한 설명: https://aimaster.tistory.com/83\n",
    "    - https://runebook.dev/ko/docs/pytorch/amp\n",
    "- 성능이 원하는 만큼 나오지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DYNAMIC QUANTIZATION\n",
    "model = SentimentClassifier(2)\n",
    "model.load_state_dict(torch.load('model/mobilebert_model_67.pt', map_location='cpu'))\n",
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STATIC QUANTIZATION"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
