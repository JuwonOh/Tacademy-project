{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'sentiment'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(tag):\n",
    "    if tag == 'irrelevant':\n",
    "        train_df = pd.read_csv('data/irrelevant_train.tsv', sep='\\t')\n",
    "        valid_df = pd.read_csv('data/irrelevant_test.tsv', sep='\\t')\n",
    "    elif tag == 'sentiment':\n",
    "        train_df = pd.read_csv('data/sentiment_train.tsv', sep='\\t')\n",
    "        valid_df = pd.read_csv('data/sentiment_test.tsv', sep='\\t')\n",
    "    else:\n",
    "        train_df = None\n",
    "        valid_df = None\n",
    "    \n",
    "    return train_df, valid_df\n",
    "\n",
    "train_df, valid_df = load_dataset(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': ['UK', 'is', 'also', 'extending', 'to', 'Hong', 'Kong', 'the', 'arms', 'embargo', 'that', 'is', 'in', 'force', 'on', 'mainland', 'China', 'since', '1989', '.'], 'sentiment': '0'}\n",
      "{'sentence': ['The', 'United', 'States', 'and', 'NATO', 'countries', 'often', 'send', 'aircraft', 'and', 'drones', 'to', 'perform', 'reconnaissance', 'activities', 'along', 'Russia', '’s', 'borders', 'in', 'the', 'Baltic', ',', 'in', 'the', 'Black', 'Sea', 'off', 'Crimea', ',', 'and', 'Krasnodar', '.'], 'sentiment': '0'}\n"
     ]
    }
   ],
   "source": [
    "## SPACY에서 제공하는 TOKENIZER를 사용\n",
    "TEXT = torchtext.legacy.data.Field(tokenize='spacy')\n",
    "LABEL = torchtext.legacy.data.LabelField()\n",
    "\n",
    "# id␞sentence␞sentiment\n",
    "fields = [(None, None),(None, None), ('sentence', TEXT),('sentiment', LABEL)]\n",
    "\n",
    "#loading custom dataset\n",
    "training_data = torchtext.legacy.data.TabularDataset(\n",
    "    path = 'data/'+tag+'_train.tsv',\n",
    "    format = 'tsv',\n",
    "    fields = fields,\n",
    "    skip_header = True\n",
    ")\n",
    "\n",
    "valid_data = torchtext.legacy.data.TabularDataset(\n",
    "    path = 'data/'+tag+'_test.tsv',\n",
    "    format = 'tsv',\n",
    "    fields = fields,\n",
    "    skip_header = True\n",
    ")\n",
    "\n",
    "# print preprocessed text\n",
    "print(vars(training_data.examples[0]))\n",
    "print(vars(valid_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = training_data.split(split_ratio=0.9, random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.6B.zip: 862MB [06:14, 2.30MB/s]                                \n",
      "100%|█████████▉| 399999/400000 [00:13<00:00, 28860.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 55820\n",
      "Size of LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "# 여러개 다양한 Glove pre-trained vector를 활용할 수 있다.\n",
    "# vectors = \"glove.42B.300d\",\n",
    "TEXT.build_vocab(train_data, vectors = \"glove.6B.100d\", unk_init = torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, test_iterator, valid_iterator = torchtext.legacy.data.BucketIterator.splits(\n",
    "    (train_data, test_data, valid_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.sentence),\n",
    "    sort_within_batch=True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        text = text.permute(1, 0)        # [batch size, sent len]        \n",
    "        embedded = self.embedding(text)  # [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1) # [batch size, 1, sent len, emb dim]\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs] # [batch size, n_filters, sent len - filter_sizes[n]]        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved] # [batch size, n_filters]\n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1)) # [batch size, n_filters * len(filter_sizes)]\n",
    "\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대부분의 hyperparameter를 최적으로 찾는 것이 중요하다\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100 # Glove Vector의 dimension과 일치하여야 한다\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [2,3,4]\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "DROPOUT = 0.4\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 5,672,902 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
       "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 1.2521,  0.5977,  0.2078,  ..., -0.9296, -0.5892, -0.3702],\n",
       "        [-0.4668,  0.7340, -1.2766,  ..., -0.5542, -0.6220, -2.8315],\n",
       "        [ 0.2032, -1.6553,  0.2869,  ...,  1.4848, -0.0791, -0.5520]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then zero the initial weights of the unknown and padding tokens.\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 중간 batch별 accuracy를 측정하기 위한 helper function\n",
    "def categorical_accuracy(preds, y):\n",
    "\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    \n",
    "    return correct.sum().detach().cpu().numpy() / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.sentence)\n",
    "        loss = criterion(predictions, batch.sentiment)\n",
    "        acc = categorical_accuracy(predictions, batch.sentiment)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.sentence)\n",
    "            loss = criterion(predictions, batch.sentiment)\n",
    "            acc = categorical_accuracy(predictions, batch.sentiment)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]: Train Loss: nan, Train Acc: 60.73% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [2/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [3/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [4/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [5/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [6/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [7/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [8/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [9/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [10/100]: Train Loss: nan, Train Acc: 60.80% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [11/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [12/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [13/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [14/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [15/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [16/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [17/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [18/100]: Train Loss: nan, Train Acc: 60.80% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [19/100]: Train Loss: nan, Train Acc: 60.91% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [20/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [21/100]: Train Loss: nan, Train Acc: 60.91% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [22/100]: Train Loss: nan, Train Acc: 60.80% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [23/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [24/100]: Train Loss: nan, Train Acc: 60.93% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [25/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [26/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [27/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [28/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [29/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [30/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [31/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [32/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [33/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [34/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [35/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [36/100]: Train Loss: nan, Train Acc: 60.80% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [37/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [38/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [39/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [40/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [41/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [42/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [43/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [44/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [45/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [46/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [47/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [48/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [49/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [50/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [51/100]: Train Loss: nan, Train Acc: 60.91% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [52/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [53/100]: Train Loss: nan, Train Acc: 60.80% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [54/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [55/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [56/100]: Train Loss: nan, Train Acc: 60.78% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [57/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [58/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [59/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [60/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [61/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [62/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [63/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [64/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [65/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [66/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [67/100]: Train Loss: nan, Train Acc: 60.80% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [68/100]: Train Loss: nan, Train Acc: 60.78% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [69/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [70/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [71/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [72/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [73/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [74/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [75/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [76/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [77/100]: Train Loss: nan, Train Acc: 60.80% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [78/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [79/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [80/100]: Train Loss: nan, Train Acc: 60.80% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [81/100]: Train Loss: nan, Train Acc: 60.93% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [82/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [83/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [84/100]: Train Loss: nan, Train Acc: 60.80% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [85/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [86/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [87/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [88/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [89/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [90/100]: Train Loss: nan, Train Acc: 60.80% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [91/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [92/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [93/100]: Train Loss: nan, Train Acc: 60.78% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [94/100]: Train Loss: nan, Train Acc: 60.85% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [95/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [96/100]: Train Loss: nan, Train Acc: 60.91% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [97/100]: Train Loss: nan, Train Acc: 60.83% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [98/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [99/100]: Train Loss: nan, Train Acc: 60.88% | Val. Loss: nan, Val. Acc: 60.93%\n",
      "Epoch [100/100]: Train Loss: nan, Train Acc: 60.80% | Val. Loss: nan, Val. Acc: 60.93%\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, test_iterator, criterion)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model/'+tag+'_CNN_model.pt')\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{EPOCHS}]: Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (embedding): Embedding(56262, 100)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 100, kernel_size=(2, 100), stride=(1, 1))\n",
       "    (1): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
       "    (2): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
       "  )\n",
       "  (fc): Linear(in_features=300, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "model.load_state_dict(torch.load('model/'+tag+'_CNN_model.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6190095656159995, 0.6658740942028986)\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(model, valid_iterator, criterion)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Get Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_class(sentence, model, min_len = 4):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    preds = model(tensor)\n",
    "    softmax_prob = torch.nn.functional.softmax(preds, dim=1)\n",
    "    max_preds = preds.argmax(dim = 1)\n",
    "    return softmax_prob, max_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4571, 0.5429]], grad_fn=<SoftmaxBackward>) tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "prob, pred_class = predict_class(\"How many minutes are in six hundred and eighteen hours?\", model)\n",
    "print(prob, pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def save_results(df, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for idx, row in tqdm(df.iterrows()):\n",
    "            news_id = row['id']\n",
    "            text = row['sentence'].replace('\\t',' ')\n",
    "            sentiment = row['sentiment']\n",
    "            class_prob, pred = predict_class(text, model)\n",
    "\n",
    "            class_prob = [str(x) for x in class_prob.detach().cpu().numpy()[0]]\n",
    "            pred = pred.detach().cpu().numpy()[0]\n",
    "\n",
    "            result = str(news_id).replace('\\t','')+'\\t'+text+'\\t'+'\\t'.join(class_prob)+'\\t'+str(pred)+'\\t'+str(int(sentiment)).replace('\\t','')\n",
    "            \n",
    "            f.write(result+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41109it [00:42, 961.78it/s] \n"
     ]
    }
   ],
   "source": [
    "save_results(train_df, 'data/'+tag+'_cnn_prediction_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17628it [00:18, 941.22it/s]\n"
     ]
    }
   ],
   "source": [
    "save_results(valid_df, 'data/'+tag+'_cnn_prediction_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
